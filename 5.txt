ENVIRONMENTAL PREPARATION FOR WEB CRAWLING
Make sure that a browser such as Chrome, IE or other has been installed in the environment.

Download and install Python

Download a suitable IDL
This article uses Visual Studio Code

Install the required Python packages
Pip is a Python package management tool. It provides functions for searching, downloading, installing, and uninstalling Python packages. 
This tool will be included when downloading and installing Python. Therefore, we can directly use ‘pip install’ to install the libraries we need.

pip install beautifulsoup4
pip install requests
pip install lxml

BeautifulSoup is a library for easily parsing HTML and XML data.
• lxml is a library to improve the parsing speed of XML files.
• requests is a library to simulate HTTP requests (such as GET and POST). We will mainly use it to access the source code of any given website.

=======================================================================================================================

1. FIRST, YOU NEED TO IMPORT THE LIBRARIES YOU NEED TO USE.
	import requests
	import lxml
	from bs4
	import BeautifulSoup

2. CREATE AND ACCESS URL
	Create a URL address that needs to be crawled, then create the header information, and then send a network request to wait for a response.

	f = requests.get(url)

	When requesting access to the content of a webpage, sometimes you will find that a 403 error will appear. This is because the server has rejected your access. 
	This is the anti-crawler setting used by the webpage to prevent malicious collection of information. At this time, you can access it by simulating the browser header information.

	f = requests.get(url, headers = headers)

3. PARSE WEBPAGE
	Create a BeautifulSoup object and specify the parser as lxml.
	soup = BeautifulSoup(f.content,'lxml')

4. EXTRACT INFORMATION
	The BeautifulSoup library has three methods to find elements:
	findall() :find all nodes
	find() :find a single node
	select() :finds according to the selector CSS Selector
	
	to get the  text
		text=soup.select('div text')

	to get the image urls
		images = soup.select('div img') 
		images_url = images[0]['src'] 
		images_url 

5. DOWNLOADING IMAGE
 
	img_data = requests.get(images_url).content 

	with open('netflix.jpg', 'wb') as handler: 

     	  handler.write(img_data) 